{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":null},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{"7ecf7158-2bb7-4175-81d9-f97de282e963":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"The cleaning spray permanently stained my beautiful kitchen counter. Never again!","2":{"response":"{\"Message\":\"FTL64 SKU Not Supported\",\"Source\":\"OPENAI\",\"error_code\":\"PERMISSION_DENIED\"}","status":{"protocolVersion":{"protocol":"HTTP","major":1,"minor":1},"statusCode":403,"reasonPhrase":"Forbidden"}}},{"0":"I used this sunscreen on my vacation to Florida, and I didn't get burned at all. Would recommend.","2":{"response":"{\"Message\":\"FTL64 SKU Not Supported\",\"Source\":\"OPENAI\",\"error_code\":\"PERMISSION_DENIED\"}","status":{"protocolVersion":{"protocol":"HTTP","major":1,"minor":1},"statusCode":403,"reasonPhrase":"Forbidden"}}},{"0":"I'm torn about this speaker system. The sound was high quality, though it didn't connect to my roommate's phone.","2":{"response":"{\"Message\":\"FTL64 SKU Not Supported\",\"Source\":\"OPENAI\",\"error_code\":\"PERMISSION_DENIED\"}","status":{"protocolVersion":{"protocol":"HTTP","major":1,"minor":1},"statusCode":403,"reasonPhrase":"Forbidden"}}},{"0":"The umbrella is OK, I guess.","2":{"response":"{\"Message\":\"FTL64 SKU Not Supported\",\"Source\":\"OPENAI\",\"error_code\":\"PERMISSION_DENIED\"}","status":{"protocolVersion":{"protocol":"HTTP","major":1,"minor":1},"statusCode":403,"reasonPhrase":"Forbidden"}}}],"schema":[{"key":"0","name":"reviews","type":"string"},{"key":"1","name":"sentiment","type":"string"},{"key":"2","name":"reviews_analyze_sentiment_error","type":"StructType(StructField(response,StringType,true),StructField(status,StructType(StructField(protocolVersion,StructType(StructField(protocol,StringType,true),StructField(major,IntegerType,false),StructField(minor,IntegerType,false)),true),StructField(statusCode,IntegerType,false),StructField(reasonPhrase,StringType,true)),true))"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["sentiment"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}}},"cells":[{"id":"a7094a53-61f4-4038-a3a3-d292a80682e2","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Transform and enrich data with AI functions\n","Microsoft Fabric AI Functions enable all business professionals (from developers to analysts) to transform and enrich their enterprise data using generative AI.\n","\n","AI functions use industry-leading large language models (LLMs) for summarization, classification, text generation, and more. With a single line of code, you can:\n","\n","- ai.analyze_sentiment: Detect the emotional state of input text.\n","- ai.classify: Categorize input text according to your labels.\n","- ai.extract: Extract specific types of information from input text (for example, locations or names).\n","- ai.fix_grammar: Correct the spelling, grammar, and punctuation of input text.\n","- ai.generate_response: Generate responses based on your own instructions.\n","- ai.similarity: Compare the meaning of input text with a single text value, or with text in another column.\n","- ai.summarize: Get summaries of input text.\n","- ai.translate: Translate input text into another language.\n","\n","You can incorporate these functions as part of data science and data engineering workflows, whether you're working with pandas or Spark. \n"]},{"id":"4e6bd476-8816-4f8e-9517-c10134fc40e0","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Import required libraries"]},{"id":"40f13121-2a4f-4618-9d61-a3b10cbf7be1","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["import synapse.ml.spark.aifunc as aifunc"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"7dd664f0-f4bd-4897-be0e-9ea0d0584ebf","normalized_state":"finished","queued_time":"2025-11-07T09:53:51.8076574Z","session_start_time":"2025-11-07T09:53:51.8085472Z","execution_start_time":"2025-11-07T09:54:03.5171176Z","execution_finish_time":"2025-11-07T09:54:07.91685Z","parent_msg_id":"b6e7edb8-9257-4be0-9cb2-0738ec3de0db"},"text/plain":"StatementMeta(, 7dd664f0-f4bd-4897-be0e-9ea0d0584ebf, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1},{"id":"c7cca250-6516-45c3-b485-f2fb9ee39149","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Apply AI functions\n","Each of the following functions allows you to invoke the built-in AI endpoint in Fabric to transform and enrich data with a single line of code. You can use AI functions to analyze pandas DataFrames or Spark DataFrames."]},{"id":"7b1fd10e-a889-48f8-a244-a35e12da9154","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Detect sentiment with ai.analyze_sentiment\n","The `ai.analyze_sentiment` function invokes AI to identify whether the emotional state expressed by input text is positive, negative, mixed, or neutral. If AI can't make this determination, the output is left blank.\n","\n","For more detailed instructions about the use of `ai.analyze_sentiment` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/analyze-sentiment)."]},{"id":"c0535a0b-5419-4e02-9f61-0d0442efbd0e","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"The cleaning spray permanently stained my beautiful kitchen counter. Never again!\",),\n","        (\"I used this sunscreen on my vacation to Florida, and I didn't get burned at all. Would recommend.\",),\n","        (\"I'm torn about this speaker system. The sound was high quality, though it didn't connect to my roommate's phone.\",),\n","        (\"The umbrella is OK, I guess.\",)\n","    ], [\"reviews\"])\n","\n","sentiment = df.ai.analyze_sentiment(input_col=\"reviews\", output_col=\"sentiment\")\n","display(sentiment)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"7dd664f0-f4bd-4897-be0e-9ea0d0584ebf","normalized_state":"finished","queued_time":"2025-11-07T09:58:27.6121641Z","session_start_time":null,"execution_start_time":"2025-11-07T09:58:27.6133281Z","execution_finish_time":"2025-11-07T09:58:59.1280951Z","parent_msg_id":"723d23e1-b8e3-47ee-b539-a72a30c1d0a4"},"text/plain":"StatementMeta(, 7dd664f0-f4bd-4897-be0e-9ea0d0584ebf, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"7ecf7158-2bb7-4175-81d9-f97de282e963","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 7ecf7158-2bb7-4175-81d9-f97de282e963)"},"metadata":{}}],"execution_count":2},{"id":"a1a2faf8-57f8-4d3d-b31b-0ac4a91f3562","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Categorize text with ai.classify\n","The `ai.classify function` invokes AI to categorize input text according to custom labels you choose. For `ai.classify` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/classify)."]},{"id":"ed209c43-ab9d-4a7b-bfec-10ac6c4d8979","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"This duvet, lovingly hand-crafted from all-natural fabric, is perfect for a good night's sleep.\",),\n","        (\"Tired of friends judging your baking? With these handy-dandy measuring cups, you'll create culinary delights.\",),\n","        (\"Enjoy this *BRAND NEW CAR!* A compact SUV perfect for the professional commuter!\",)\n","    ], [\"descriptions\"])\n","    \n","categories = df.ai.classify(labels=[\"kitchen\", \"bedroom\", \"garage\", \"other\"], input_col=\"descriptions\", output_col=\"categories\")\n","display(categories)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"cancelled","session_id":"7dd664f0-f4bd-4897-be0e-9ea0d0584ebf","normalized_state":"cancelled","queued_time":"2025-11-07T10:02:12.4077655Z","session_start_time":null,"execution_start_time":"2025-11-07T10:02:12.4088545Z","execution_finish_time":"2025-11-07T10:02:18.2772409Z","parent_msg_id":"0dd7f47d-0662-4744-b0a3-6b7ff6f5a037"},"text/plain":"StatementMeta(, 7dd664f0-f4bd-4897-be0e-9ea0d0584ebf, 5, Finished, Cancelled, Cancelled)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython.\n: org.apache.spark.SparkException: Job 12 cancelled part of cancelled job group 5\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3102)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1248)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3240)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3229)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2584)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2605)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2624)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:555)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4358)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:324)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:263)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:254)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3550)\n\tat org.apache.spark.sql.GetRowsHelper$.getRowsInJsonString(GetRowsHelper.scala:51)\n\tat com.microsoft.spark.notebook.visualization.display$.generateTableConfig(Display.scala:422)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:230)\n\tat com.microsoft.spark.notebook.visualization.display$.$anonfun$getDisplayResultInternal$1(Display.scala:190)\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultInternal(Display.scala:180)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultForIPython(Display.scala:99)\n\tat com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython(Display.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis duvet, lovingly hand-crafted from all-natural fabric, is perfect for a good night\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms sleep.\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m      6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTired of friends judging your baking? With these handy-dandy measuring cups, you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll create culinary delights.\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnjoy this *BRAND NEW CAR!* A compact SUV perfect for the professional commuter!\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m      8\u001b[0m     ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m categories \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mai\u001b[38;5;241m.\u001b[39mclassify(labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkitchen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedroom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgarage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m], input_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m display(categories)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/notebookutils/visualization/display.py:278\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(data, summary)\u001b[0m\n\u001b[1;32m    275\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     log4jLogger \\\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay failed with error, language: python, error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, correlationId=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     duration_ms \u001b[38;5;241m=\u001b[39m ceil((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/notebookutils/visualization/display.py:270\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(data, summary)\u001b[0m\n\u001b[1;32m    267\u001b[0m             _clear()\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _display_with_spark(data, correlation_id, summary)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/notebookutils/visualization/display.py:296\u001b[0m, in \u001b[0;36m_display_with_spark\u001b[0;34m(data, correlation_id, summary)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ipython_enabled(runtime\u001b[38;5;241m.\u001b[39mhost_nbutils_version):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# pylint: disable=C0415\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m publish_display_data\n\u001b[0;32m--> 296\u001b[0m     display_str \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mgetDisplayResultForIPython(\n\u001b[1;32m    297\u001b[0m             df\u001b[38;5;241m.\u001b[39m_jdf, summary, correlation_id)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_str:\n\u001b[1;32m    299\u001b[0m         display_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(display_str)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython.\n: org.apache.spark.SparkException: Job 12 cancelled part of cancelled job group 5\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3102)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1248)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3240)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3229)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2584)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2605)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2624)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:555)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4358)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4348)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4346)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:324)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:263)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:254)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4346)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3327)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3550)\n\tat org.apache.spark.sql.GetRowsHelper$.getRowsInJsonString(GetRowsHelper.scala:51)\n\tat com.microsoft.spark.notebook.visualization.display$.generateTableConfig(Display.scala:422)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:230)\n\tat com.microsoft.spark.notebook.visualization.display$.$anonfun$getDisplayResultInternal$1(Display.scala:190)\n\tat com.microsoft.spark.notebook.common.trident.CertifiedTelemetryUtils$.withTelemetry(CertifiedTelemetryUtils.scala:82)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultInternal(Display.scala:180)\n\tat com.microsoft.spark.notebook.visualization.display$.getDisplayResultForIPython(Display.scala:99)\n\tat com.microsoft.spark.notebook.visualization.display.getDisplayResultForIPython(Display.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"execution_count":3},{"id":"5fd4fee6-8a30-44ff-aca0-fab00b7550a5","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Extract entities with ai.extract\n","The `ai.extract` function invokes AI to scan input text and extract specific types of information that are designated by labels you choose (for example, locations or names). \n","\n","For more detailed instructions about the use of `ai.extract` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/extract)."]},{"id":"a631bcfd-3665-49db-8e4b-59a820c52e82","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"MJ Lee lives in Tucson, AZ, and works as a software engineer for Microsoft.\",),\n","        (\"Kris Turner, a nurse at NYU Langone, is a resident of Jersey City, New Jersey.\",)\n","    ], [\"descriptions\"])\n","\n","df_entities = df.ai.extract(labels=[\"name\", \"profession\", \"city\"], input_col=\"descriptions\")\n","display(df_entities)"],"outputs":[]},{"id":"17378a51-734b-4ceb-b9ad-1c2cf1458286","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Fix grammar with ai.fix_grammar\n","The `ai.fix_grammar` function invokes AI to correct the spelling, grammar, and punctuation of input text. \n","\n","For more detailed instructions about the use of `ai.fix_grammar` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/fix-grammar)."]},{"id":"641d81f0-ce7d-4dd2-98ee-bade7bd10069","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"There are an error here.\",),\n","        (\"She and me go weigh back. We used to hang out every weeks.\",),\n","        (\"The big picture are right, but you're details is all wrong.\",)\n","    ], [\"text\"])\n","\n","corrections = df.ai.fix_grammar(input_col=\"text\", output_col=\"corrections\")\n","display(corrections)"],"outputs":[]},{"id":"2acdb2b9-f30a-4bc7-a7a0-21bde7c93b55","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Answer custom user prompts with ai.generate_response\n","The `ai.generate_response` function invokes AI to generate custom text based on your own instructions. \n","\n","For more detailed instructions about the use of `ai.generate_response` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/generate-response)."]},{"id":"b470d91f-4f82-4bcc-a614-140ef8024798","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"Scarves\",),\n","        (\"Snow pants\",),\n","        (\"Ski goggles\",)\n","    ], [\"product\"])\n","\n","responses = df.ai.generate_response(prompt=\"Write a short, punchy email subject line for a winter sale.\", output_col=\"response\")\n","display(responses)"],"outputs":[]},{"id":"094c84fe-a160-469a-bf81-28cc4ce920ba","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Calculate similarity with ai.similarity\n","The `ai.similarity` function compares each input text value either to one common reference text or to the corresponding value in another column (pairwise mode). The output similarity score values are relative, and they can range from -1 (opposites) to 1 (identical). A score of 0 indicates that the values are unrelated in meaning. \n","\n","For more detailed instructions about the use of `ai.similarity` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/similarity)."]},{"id":"f0752405-2846-4b68-9fac-7087b6a04121","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"Bill Gates\", \"Technology\"), \n","        (\"Satya Nadella\", \"Healthcare\"), \n","        (\"Joan of Arc\", \"Agriculture\")\n","    ], [\"names\", \"industries\"])\n","\n","similarity = df.ai.similarity(input_col=\"names\", other_col=\"industries\", output_col=\"similarity\")\n","display(similarity)"],"outputs":[]},{"id":"1f621db0-6b81-4bfa-8b3c-5da6d64c9e8e","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Summarize text with ai.summarize\n","The `ai.summarize` function invokes AI to generate summaries of input text (either values from a single column of a DataFrame, or row values across all the columns). \n","\n","For more detailed instructions about the use of `ai.summarize` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/summarize)."]},{"id":"075f86b3-b054-4a74-83a6-3604567a767e","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"Microsoft Teams\", \"2017\",\n","        \"\"\"\n","        The ultimate messaging app for your organizationâ€”a workspace for real-time \n","        collaboration and communication, meetings, file and app sharing, and even the \n","        occasional emoji! All in one place, all in the open, all accessible to everyone.\n","        \"\"\",),\n","        (\"Microsoft Fabric\", \"2023\",\n","        \"\"\"\n","        An enterprise-ready, end-to-end analytics platform that unifies data movement, \n","        data processing, ingestion, transformation, and report building into a seamless, \n","        user-friendly SaaS experience. Transform raw data into actionable insights.\n","        \"\"\",)\n","    ], [\"product\", \"release_year\", \"description\"])\n","\n","summaries = df.ai.summarize(input_col=\"description\", output_col=\"summary\")\n","display(summaries)"],"outputs":[]},{"id":"d0cd58c2-b92a-4e6e-a911-8dc05f3c84a1","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["### Translate text with ai.translate\n","The `ai.translate` function invokes AI to translate input text to a new language of your choice. \n","\n","For more detailed instructions about the use of `ai.translate` with PySpark, [see this article](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pyspark/translate)."]},{"id":"d6920ddd-88f5-4fb2-b266-99c807cdee4a","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# This code uses AI. Always review output for mistakes. \n","# Read terms: https://azure.microsoft.com/support/legal/preview-supplemental-terms/.\n","\n","df = spark.createDataFrame([\n","        (\"Hello! How are you doing today?\",),\n","        (\"Tell me what you'd like to know, and I'll do my best to help.\",),\n","        (\"The only thing we have to fear is fear itself.\",),\n","    ], [\"text\"])\n","\n","translations = df.ai.translate(to_lang=\"spanish\", input_col=\"text\", output_col=\"translations\")\n","display(translations)"],"outputs":[]}]}